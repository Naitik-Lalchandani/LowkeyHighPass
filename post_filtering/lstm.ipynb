{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#FILE ORGANIZATION\n",
        "# dereverberation_dataset/\n",
        "# ‚îú‚îÄ‚îÄ train/\n",
        "# ‚îÇ   ‚îú‚îÄ‚îÄ clean/          (folder)\n",
        "# ‚îÇ   ‚îî‚îÄ‚îÄ reverberant/    (folder)\n",
        "# ‚îú‚îÄ‚îÄ test/\n",
        "# ‚îÇ   ‚îú‚îÄ‚îÄ output/         (folder)\n",
        "# ‚îÇ   ‚îî‚îÄ‚îÄ reverberant/    (folder - EMPTY)\n",
        "# ‚îî‚îÄ‚îÄ dereverberation_checkpoints/\n",
        "#     ‚îú‚îÄ‚îÄ best_model.pt\n",
        "#     ‚îî‚îÄ‚îÄ checkpoint_epoch_X.pt"
      ],
      "metadata": {
        "id": "h7WeE4rYMFJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L51BnN79Ordx",
        "outputId": "cd5aae16-2274-4088-a2a7-d20f5379376e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "UNIFIED Steps 1 & 2: Audio Preprocessing for Training\n",
            "============================================================\n",
            "Running in Google Colab\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "üìÅ Using Google Drive path: /content/drive/MyDrive/dereverberation_dataset\n",
            "   Dataset structure:\n",
            "   /content/drive/MyDrive/dereverberation_dataset/\n",
            "     ‚îú‚îÄ‚îÄ train/\n",
            "     ‚îÇ   ‚îú‚îÄ‚îÄ clean/\n",
            "     ‚îÇ   ‚îî‚îÄ‚îÄ reverberant/\n",
            "     ‚îî‚îÄ‚îÄ test/\n",
            "         ‚îî‚îÄ‚îÄ reverberant/\n",
            "\n",
            "‚úÖ Found 2 clean training files\n",
            "‚úÖ Found 2 reverberant training files\n",
            "‚ö†Ô∏è  Test folder exists but no audio files found\n",
            "Initialized AudioPreprocessor:\n",
            "  Sample rate: 16000 Hz\n",
            "  Frame length: 512 samples (32 ms)\n",
            "  Hop length: 128 samples (8 ms)\n",
            "  FFT size: 512\n",
            "  Frequency bins: 257\n",
            "\n",
            "============================================================\n",
            "STEP 1: Computing normalization statistics\n",
            "============================================================\n",
            "Computing normalization statistics from 2 reverberant audio files...\n",
            "Normalization statistics computed successfully!\n",
            "  Mean shape: (1, 257)\n",
            "  Mean range: [0.2559, 1.4560]\n",
            "  Std shape: (1, 257)\n",
            "  Std range: [0.0672, 0.7227]\n",
            "\n",
            "============================================================\n",
            "STEP 2: Generating training pairs\n",
            "============================================================\n",
            "  Processed 2/2 training pairs\n",
            "\n",
            "‚úÖ Training dataset generated successfully!\n",
            "  Total training pairs: 2\n",
            "  Input shape (each): (751, 257) (normalized)\n",
            "  Target shape (each): (751, 257) (unnormalized)\n",
            "\n",
            "============================================================\n",
            "PROCESSING COMPLETE\n",
            "============================================================\n",
            "‚úì Normalization stats computed from reverberant training audio\n",
            "‚úì Training pairs created:\n",
            "  - Input: Normalized reverberant features\n",
            "  - Target: Unnormalized clean features\n",
            "‚úì Ready for Step 3 (Model Training)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from scipy import signal\n",
        "from scipy.fft import rfft, irfft\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "class AudioPreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocessing pipeline for LSTM-based dereverberation.\n",
        "    Implements Step 1 from the paper.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 sample_rate=16000,\n",
        "                 frame_length_ms=32,\n",
        "                 frame_shift_ms=8,\n",
        "                 n_fft=512,\n",
        "                 normalize=True):\n",
        "        \"\"\"\n",
        "        Initialize the audio preprocessor.\n",
        "\n",
        "        Args:\n",
        "            sample_rate: Audio sampling rate (Hz)\n",
        "            frame_length_ms: Frame length in milliseconds (32ms as per paper)\n",
        "            frame_shift_ms: Frame shift/hop in milliseconds (8ms as per paper)\n",
        "            n_fft: FFT size (512 points as per paper)\n",
        "            normalize: Whether to normalize features\n",
        "        \"\"\"\n",
        "        self.sample_rate = sample_rate\n",
        "        self.frame_length_ms = frame_length_ms\n",
        "        self.frame_shift_ms = frame_shift_ms\n",
        "        self.n_fft = n_fft\n",
        "        self.normalize = normalize\n",
        "\n",
        "        # Convert ms to samples\n",
        "        self.frame_length = int(frame_length_ms * sample_rate / 1000)  # 512 samples at 16kHz\n",
        "        self.hop_length = int(frame_shift_ms * sample_rate / 1000)      # 128 samples at 16kHz\n",
        "\n",
        "        # Number of frequency bins (257 for 512-point FFT)\n",
        "        self.n_freq_bins = n_fft // 2 + 1\n",
        "\n",
        "        # Statistics for normalization (to be computed from training data)\n",
        "        self.feature_mean = None\n",
        "        self.feature_std = None\n",
        "\n",
        "        print(f\"Initialized AudioPreprocessor:\")\n",
        "        print(f\"  Sample rate: {sample_rate} Hz\")\n",
        "        print(f\"  Frame length: {self.frame_length} samples ({frame_length_ms} ms)\")\n",
        "        print(f\"  Hop length: {self.hop_length} samples ({frame_shift_ms} ms)\")\n",
        "        print(f\"  FFT size: {n_fft}\")\n",
        "        print(f\"  Frequency bins: {self.n_freq_bins}\")\n",
        "\n",
        "    def load_audio(self, audio_path):\n",
        "        \"\"\"\n",
        "        Load audio file and resample to target sample rate.\n",
        "\n",
        "        Args:\n",
        "            audio_path: Path to audio file\n",
        "\n",
        "        Returns:\n",
        "            audio: Audio time series\n",
        "        \"\"\"\n",
        "        audio, sr = librosa.load(audio_path, sr=self.sample_rate, mono=True)\n",
        "        return audio\n",
        "\n",
        "    def extract_magnitude_spectrum(self, audio):\n",
        "        \"\"\"\n",
        "        Extract magnitude spectrum using STFT with Hamming window.\n",
        "\n",
        "        Args:\n",
        "            audio: Input audio signal (1D numpy array)\n",
        "\n",
        "        Returns:\n",
        "            magnitude: Magnitude spectrum (n_frames, n_freq_bins)\n",
        "            phase: Phase spectrum (n_frames, n_freq_bins)\n",
        "        \"\"\"\n",
        "        # Apply STFT with Hamming window\n",
        "        stft_matrix = librosa.stft(\n",
        "            audio,\n",
        "            n_fft=self.n_fft,\n",
        "            hop_length=self.hop_length,\n",
        "            win_length=self.frame_length,\n",
        "            window='hamming',\n",
        "            center=True,\n",
        "            pad_mode='reflect'\n",
        "        )\n",
        "\n",
        "        # Extract magnitude and phase\n",
        "        magnitude = np.abs(stft_matrix).T  # Shape: (n_frames, n_freq_bins)\n",
        "        phase = np.angle(stft_matrix).T     # Shape: (n_frames, n_freq_bins)\n",
        "\n",
        "        return magnitude, phase\n",
        "\n",
        "    def apply_cubic_root_compression(self, magnitude):\n",
        "        \"\"\"\n",
        "        Apply cubic root compression to magnitude spectrum.\n",
        "\n",
        "        Args:\n",
        "            magnitude: Magnitude spectrum (n_frames, n_freq_bins)\n",
        "\n",
        "        Returns:\n",
        "            compressed: Cubic root compressed magnitude\n",
        "        \"\"\"\n",
        "        # Cubic root compression: Y_compressed = Y^(1/3)\n",
        "        compressed = np.power(magnitude, 1.0/3.0)\n",
        "        return compressed\n",
        "\n",
        "    def normalize_features(self, features, compute_stats=False):\n",
        "        \"\"\"\n",
        "        Normalize features to zero mean and unit variance.\n",
        "\n",
        "        Args:\n",
        "            features: Input features (n_frames, n_freq_bins)\n",
        "            compute_stats: If True, compute and store mean/std from this data\n",
        "\n",
        "        Returns:\n",
        "            normalized: Normalized features\n",
        "        \"\"\"\n",
        "        if compute_stats:\n",
        "            # Compute statistics across all frames and frequency bins\n",
        "            self.feature_mean = np.mean(features, axis=0, keepdims=True)\n",
        "            self.feature_std = np.std(features, axis=0, keepdims=True)\n",
        "            # Avoid division by zero\n",
        "            self.feature_std = np.maximum(self.feature_std, 1e-8)\n",
        "            print(f\"Computed normalization statistics:\")\n",
        "            print(f\"  Mean shape: {self.feature_mean.shape}\")\n",
        "            print(f\"  Std shape: {self.feature_std.shape}\")\n",
        "\n",
        "        if self.feature_mean is None or self.feature_std is None:\n",
        "            raise ValueError(\"Normalization statistics not computed. Set compute_stats=True first.\")\n",
        "\n",
        "        # Normalize\n",
        "        normalized = (features - self.feature_mean) / self.feature_std\n",
        "        return normalized\n",
        "\n",
        "    def process_audio(self, audio, compute_stats=False, return_phase=True):\n",
        "        \"\"\"\n",
        "        Complete preprocessing pipeline for a single audio signal.\n",
        "\n",
        "        Args:\n",
        "            audio: Input audio signal (1D numpy array) or path to audio file\n",
        "            compute_stats: If True, compute normalization statistics\n",
        "            return_phase: If True, return phase information\n",
        "\n",
        "        Returns:\n",
        "            features: Preprocessed features (n_frames, n_freq_bins)\n",
        "            phase: Phase spectrum (if return_phase=True)\n",
        "        \"\"\"\n",
        "        # Load audio if path is provided\n",
        "        if isinstance(audio, str):\n",
        "            audio = self.load_audio(audio)\n",
        "\n",
        "        # Step 1: Extract magnitude spectrum\n",
        "        magnitude, phase = self.extract_magnitude_spectrum(audio)\n",
        "\n",
        "        # Step 2: Apply cubic root compression\n",
        "        compressed = self.apply_cubic_root_compression(magnitude)\n",
        "\n",
        "        # Step 3: Normalize (if enabled)\n",
        "        if self.normalize:\n",
        "            features = self.normalize_features(compressed, compute_stats=compute_stats)\n",
        "        else:\n",
        "            features = compressed\n",
        "\n",
        "        if return_phase:\n",
        "            return features, phase\n",
        "        else:\n",
        "            return features\n",
        "\n",
        "    def compute_normalization_stats_from_dataset(self, audio_list):\n",
        "        \"\"\"\n",
        "        Compute normalization statistics from a list of audio files/arrays.\n",
        "        This should be called on ALL REVERBERANT training audio files.\n",
        "\n",
        "        Args:\n",
        "            audio_list: List of audio file paths or numpy arrays (reverberant only)\n",
        "        \"\"\"\n",
        "        all_features = []\n",
        "\n",
        "        print(f\"Computing normalization statistics from {len(audio_list)} reverberant audio files...\")\n",
        "        for i, audio in enumerate(audio_list):\n",
        "            if isinstance(audio, str):\n",
        "                audio = self.load_audio(audio)\n",
        "\n",
        "            magnitude, _ = self.extract_magnitude_spectrum(audio)\n",
        "            compressed = self.apply_cubic_root_compression(magnitude)\n",
        "            all_features.append(compressed)\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"  Processed {i + 1}/{len(audio_list)} files\")\n",
        "\n",
        "        # Concatenate all features and compute global statistics\n",
        "        all_features = np.concatenate(all_features, axis=0)\n",
        "        self.feature_mean = np.mean(all_features, axis=0, keepdims=True)\n",
        "        self.feature_std = np.std(all_features, axis=0, keepdims=True)\n",
        "        self.feature_std = np.maximum(self.feature_std, 1e-8)\n",
        "\n",
        "        print(f\"Normalization statistics computed successfully!\")\n",
        "        print(f\"  Mean shape: {self.feature_mean.shape}\")\n",
        "        print(f\"  Mean range: [{self.feature_mean.min():.4f}, {self.feature_mean.max():.4f}]\")\n",
        "        print(f\"  Std shape: {self.feature_std.shape}\")\n",
        "        print(f\"  Std range: [{self.feature_std.min():.4f}, {self.feature_std.max():.4f}]\")\n",
        "\n",
        "\n",
        "class TrainingTargetGenerator:\n",
        "    \"\"\"\n",
        "    UNIFIED preprocessing for training pairs.\n",
        "    Handles normalization correctly: normalize reverb (input), keep clean unnormalized (target).\n",
        "    \"\"\"\n",
        "    def __init__(self, preprocessor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            preprocessor: AudioPreprocessor instance with computed normalization stats\n",
        "        \"\"\"\n",
        "        self.preprocessor = preprocessor\n",
        "        self.sample_rate = preprocessor.sample_rate\n",
        "        self.n_fft = preprocessor.n_fft\n",
        "        self.frame_length = preprocessor.frame_length\n",
        "        self.hop_length = preprocessor.hop_length\n",
        "        self.n_freq_bins = preprocessor.n_freq_bins\n",
        "\n",
        "    def generate_training_pair_from_real_data(self, clean_audio_path, reverb_audio_path):\n",
        "        \"\"\"\n",
        "        Process clean + reverberant pair for training.\n",
        "        IMPORTANT: Returns NORMALIZED reverb features and UNNORMALIZED clean features.\n",
        "\n",
        "        Args:\n",
        "            clean_audio_path: Path to clean/dry audio file\n",
        "            reverb_audio_path: Path to reverberant audio file\n",
        "\n",
        "        Returns:\n",
        "            reverb_features: NORMALIZED cubic-root compressed reverberant features (input)\n",
        "            target_features: UNNORMALIZED cubic-root compressed clean features (target)\n",
        "            reverb_phase: Phase from reverberant audio (for reconstruction)\n",
        "        \"\"\"\n",
        "        # Load both audios\n",
        "        clean_audio = self.preprocessor.load_audio(clean_audio_path)\n",
        "        reverb_audio = self.preprocessor.load_audio(reverb_audio_path)\n",
        "\n",
        "        # Ensure same length (truncate to shorter one)\n",
        "        min_len = min(len(clean_audio), len(reverb_audio))\n",
        "        clean_audio = clean_audio[:min_len]\n",
        "        reverb_audio = reverb_audio[:min_len]\n",
        "\n",
        "        # Extract magnitude spectra\n",
        "        clean_mag, _ = self.preprocessor.extract_magnitude_spectrum(clean_audio)\n",
        "        reverb_mag, reverb_phase = self.preprocessor.extract_magnitude_spectrum(reverb_audio)\n",
        "\n",
        "        # Apply cubic root compression to both\n",
        "        clean_compressed = self.preprocessor.apply_cubic_root_compression(clean_mag)\n",
        "        reverb_compressed = self.preprocessor.apply_cubic_root_compression(reverb_mag)\n",
        "\n",
        "        # CRITICAL: Normalize ONLY reverb features (input to LSTM)\n",
        "        # Clean features stay unnormalized (target for LSTM)\n",
        "        if self.preprocessor.feature_mean is None:\n",
        "            raise ValueError(\"Preprocessor must have normalization stats computed before processing pairs!\")\n",
        "\n",
        "        reverb_features = self.preprocessor.normalize_features(reverb_compressed, compute_stats=False)\n",
        "        target_features = clean_compressed  # Keep clean unnormalized\n",
        "\n",
        "        return reverb_features, target_features, reverb_phase\n",
        "\n",
        "    def save_audio_examples(self, clean_path, reverb_path, output_prefix='step2_example'):\n",
        "        \"\"\"Save the original audio files for comparison\"\"\"\n",
        "        clean_audio = self.preprocessor.load_audio(clean_path)\n",
        "        reverb_audio = self.preprocessor.load_audio(reverb_path)\n",
        "\n",
        "        sf.write(f'{output_prefix}_clean.wav', clean_audio, self.sample_rate)\n",
        "        sf.write(f'{output_prefix}_reverberant.wav', reverb_audio, self.sample_rate)\n",
        "\n",
        "\n",
        "class RealDataPreparer:\n",
        "    \"\"\"\n",
        "    Prepares real-world clean + reverberant pairs for training.\n",
        "    Handles normalization statistics computation and pair processing.\n",
        "    \"\"\"\n",
        "    def __init__(self, sample_rate=16000):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.preprocessor = AudioPreprocessor(sample_rate=sample_rate, normalize=True)\n",
        "\n",
        "    def prepare_dataset_from_folder(self, data_folder):\n",
        "        \"\"\"\n",
        "        Expects folder structure:\n",
        "        data_folder/\n",
        "            clean/\n",
        "                file1.wav, file2.wav, ...\n",
        "            reverberant/\n",
        "                file1.wav, file2.wav, ...\n",
        "\n",
        "        Returns:\n",
        "            reverb_features_list: List of normalized reverb features (inputs)\n",
        "            target_features_list: List of unnormalized clean features (targets)\n",
        "        \"\"\"\n",
        "        clean_dir = os.path.join(data_folder, 'clean')\n",
        "        reverb_dir = os.path.join(data_folder, 'reverberant')\n",
        "\n",
        "        clean_files = sorted([f for f in os.listdir(clean_dir) if f.endswith('.wav')])\n",
        "        reverb_files = sorted([f for f in os.listdir(reverb_dir) if f.endswith('.wav')])\n",
        "\n",
        "        print(f\"Found {len(clean_files)} clean files and {len(reverb_files)} reverberant files\")\n",
        "\n",
        "        # STEP 1: Compute normalization stats from ALL reverberant files\n",
        "        reverb_paths = [os.path.join(reverb_dir, f) for f in reverb_files]\n",
        "        self.preprocessor.compute_normalization_stats_from_dataset(reverb_paths)\n",
        "\n",
        "        # STEP 2: Process all pairs with computed normalization stats\n",
        "        reverb_features_list = []\n",
        "        target_features_list = []\n",
        "\n",
        "        target_gen = TrainingTargetGenerator(preprocessor=self.preprocessor)\n",
        "\n",
        "        print(f\"\\nProcessing {len(clean_files)} training pairs...\")\n",
        "        for i, (clean_file, reverb_file) in enumerate(zip(clean_files, reverb_files)):\n",
        "            clean_path = os.path.join(clean_dir, clean_file)\n",
        "            reverb_path = os.path.join(reverb_dir, reverb_file)\n",
        "\n",
        "            # Generate training pair (normalized reverb + unnormalized clean)\n",
        "            rev_feat, target_feat, _ = target_gen.generate_training_pair_from_real_data(\n",
        "                clean_path, reverb_path\n",
        "            )\n",
        "\n",
        "            reverb_features_list.append(rev_feat)\n",
        "            target_features_list.append(target_feat)\n",
        "\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"  Processed {i + 1}/{len(clean_files)} pairs\")\n",
        "\n",
        "        print(f\"‚úì Dataset preparation complete!\")\n",
        "        print(f\"  Total pairs: {len(reverb_features_list)}\")\n",
        "        print(f\"  Input (reverb): normalized, cubic-root compressed\")\n",
        "        print(f\"  Target (clean): unnormalized, cubic-root compressed\")\n",
        "\n",
        "        return reverb_features_list, target_features_list\n",
        "\n",
        "\n",
        "# === STEP 1 & 2 EXECUTION - UNIFIED AND CORRECTED ===\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"UNIFIED Steps 1 & 2: Audio Preprocessing for Training\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Check if running in Google Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "        print(\"Running in Google Colab\")\n",
        "    except:\n",
        "        IN_COLAB = False\n",
        "        print(\"Running locally\")\n",
        "\n",
        "    # Mount Google Drive if in Colab\n",
        "    if IN_COLAB:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        # MODIFY THIS PATH to match your Google Drive structure\n",
        "        drive_base = '/content/drive/MyDrive/dereverberation_dataset'\n",
        "        train_folder = os.path.join(drive_base, 'train')\n",
        "        test_folder = os.path.join(drive_base, 'test')\n",
        "\n",
        "        print(f\"\\nüìÅ Using Google Drive path: {drive_base}\")\n",
        "        print(f\"   Dataset structure:\")\n",
        "        print(f\"   {drive_base}/\")\n",
        "        print(f\"     ‚îú‚îÄ‚îÄ train/\")\n",
        "        print(f\"     ‚îÇ   ‚îú‚îÄ‚îÄ clean/\")\n",
        "        print(f\"     ‚îÇ   ‚îî‚îÄ‚îÄ reverberant/\")\n",
        "        print(f\"     ‚îî‚îÄ‚îÄ test/\")\n",
        "        print(f\"         ‚îî‚îÄ‚îÄ reverberant/\")\n",
        "    else:\n",
        "        # Local paths\n",
        "        train_folder = './train'\n",
        "        test_folder = './test'\n",
        "        print(f\"\\nüìÅ Using local paths: ./train and ./test\")\n",
        "\n",
        "    train_clean_dir = os.path.join(train_folder, 'clean')\n",
        "    train_reverb_dir = os.path.join(train_folder, 'reverberant')\n",
        "    test_reverb_dir = os.path.join(test_folder, 'reverberant')\n",
        "\n",
        "    # Check if train directories exist\n",
        "    if not os.path.exists(train_clean_dir) or not os.path.exists(train_reverb_dir):\n",
        "        print(f\"\\n‚ùå ERROR: Required training directory structure not found!\")\n",
        "        print(f\"\\nExpected structure:\")\n",
        "        if IN_COLAB:\n",
        "            print(f\"  /content/drive/MyDrive/dereverberation_dataset/\")\n",
        "            print(f\"    ‚îú‚îÄ‚îÄ train/\")\n",
        "            print(f\"    ‚îÇ   ‚îú‚îÄ‚îÄ clean/      (your clean/dry audio files)\")\n",
        "            print(f\"    ‚îÇ   ‚îî‚îÄ‚îÄ reverberant/ (your reverberant audio files)\")\n",
        "            print(f\"    ‚îî‚îÄ‚îÄ test/           (optional)\")\n",
        "            print(f\"        ‚îî‚îÄ‚îÄ reverberant/ (reverberant test files - outputs will be generated)\")\n",
        "            print(f\"\\nüí° TIP: Upload your dataset to Google Drive first!\")\n",
        "            print(f\"   Then modify 'drive_base' variable in the code to match your path.\")\n",
        "        else:\n",
        "            print(f\"  ./train/\")\n",
        "            print(f\"    ‚îú‚îÄ‚îÄ clean/      (your clean/dry audio files)\")\n",
        "            print(f\"    ‚îî‚îÄ‚îÄ reverberant/ (your reverberant audio files)\")\n",
        "            print(f\"  ./test/           (optional)\")\n",
        "            print(f\"    ‚îî‚îÄ‚îÄ reverberant/ (reverberant test files - outputs will be generated)\")\n",
        "        print(f\"\\nPlease create this structure and place your audio files accordingly.\")\n",
        "        exit(1)\n",
        "\n",
        "    # Check if test directory exists (only reverberant folder needed)\n",
        "    has_test_data = os.path.exists(test_reverb_dir)\n",
        "\n",
        "    # Get file lists\n",
        "    clean_files = sorted([f for f in os.listdir(train_clean_dir) if f.lower().endswith(('.wav', '.flac', '.mp3', '.m4a', '.ogg'))])\n",
        "    reverb_files = sorted([f for f in os.listdir(train_reverb_dir) if f.lower().endswith(('.wav', '.flac', '.mp3', '.m4a', '.ogg'))])\n",
        "\n",
        "    if not clean_files or not reverb_files:\n",
        "        print(f\"‚ùå ERROR: No audio files found in train/clean/ or train/reverberant/\")\n",
        "        exit(1)\n",
        "\n",
        "    print(f\"\\n‚úÖ Found {len(clean_files)} clean training files\")\n",
        "    print(f\"‚úÖ Found {len(reverb_files)} reverberant training files\")\n",
        "\n",
        "    if has_test_data:\n",
        "        test_reverb_files = sorted([f for f in os.listdir(test_reverb_dir) if f.lower().endswith(('.wav', '.flac', '.mp3', '.m4a', '.ogg'))])\n",
        "\n",
        "        if test_reverb_files:\n",
        "            print(f\"‚úÖ Found {len(test_reverb_files)} reverberant test files (for inference)\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Test folder exists but no audio files found\")\n",
        "            has_test_data = False\n",
        "    else:\n",
        "        print(f\"‚ÑπÔ∏è  No test dataset found (optional)\")\n",
        "        test_reverb_files = []\n",
        "\n",
        "    # Initialize preprocessor\n",
        "    preprocessor = AudioPreprocessor(\n",
        "        sample_rate=16000,\n",
        "        frame_length_ms=32,\n",
        "        frame_shift_ms=8,\n",
        "        n_fft=512,\n",
        "        normalize=True\n",
        "    )\n",
        "\n",
        "    # STEP 1: Compute normalization statistics from ALL reverberant audio\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 1: Computing normalization statistics\")\n",
        "    print(\"=\"*60)\n",
        "    reverb_paths = [os.path.join(train_reverb_dir, f) for f in reverb_files]\n",
        "    preprocessor.compute_normalization_stats_from_dataset(reverb_paths)\n",
        "\n",
        "    # STEP 2: Generate training pairs\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 2: Generating training pairs\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    target_gen = TrainingTargetGenerator(preprocessor=preprocessor)\n",
        "    reverb_features_list = []\n",
        "    target_features_list = []\n",
        "    phase_list = []\n",
        "\n",
        "    try:\n",
        "        for i, (clean_file, reverb_file) in enumerate(zip(clean_files, reverb_files)):\n",
        "            clean_path = os.path.join(train_clean_dir, clean_file)\n",
        "            reverb_path = os.path.join(train_reverb_dir, reverb_file)\n",
        "\n",
        "            reverb_features, target_features, reverb_phase = target_gen.generate_training_pair_from_real_data(\n",
        "                clean_path, reverb_path\n",
        "            )\n",
        "\n",
        "            reverb_features_list.append(reverb_features)\n",
        "            target_features_list.append(target_features)\n",
        "            phase_list.append(reverb_phase)\n",
        "\n",
        "            if (i + 1) % 10 == 0 or (i + 1) == len(clean_files):\n",
        "                print(f\"  Processed {i + 1}/{len(clean_files)} training pairs\")\n",
        "\n",
        "        print(f\"\\n‚úÖ Training dataset generated successfully!\")\n",
        "        print(f\"  Total training pairs: {len(reverb_features_list)}\")\n",
        "        print(f\"  Input shape (each): {reverb_features_list[0].shape} (normalized)\")\n",
        "        print(f\"  Target shape (each): {target_features_list[0].shape} (unnormalized)\")\n",
        "\n",
        "        # Process test dataset if available (inference only - no targets)\n",
        "        if has_test_data:\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"STEP 3: Processing test data (inference)\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            test_reverb_features_list = []\n",
        "            test_phase_list = []\n",
        "            test_filenames = []\n",
        "\n",
        "            for i, reverb_file in enumerate(test_reverb_files):\n",
        "                reverb_path = os.path.join(test_reverb_dir, reverb_file)\n",
        "\n",
        "                # Load and process test audio\n",
        "                reverb_audio = preprocessor.load_audio(reverb_path)\n",
        "                reverb_mag, reverb_phase = preprocessor.extract_magnitude_spectrum(reverb_audio)\n",
        "                reverb_compressed = preprocessor.apply_cubic_root_compression(reverb_mag)\n",
        "                reverb_features = preprocessor.normalize_features(reverb_compressed, compute_stats=False)\n",
        "\n",
        "                test_reverb_features_list.append(reverb_features)\n",
        "                test_phase_list.append(reverb_phase)\n",
        "                test_filenames.append(reverb_file)\n",
        "\n",
        "                if (i + 1) % 10 == 0 or (i + 1) == len(test_reverb_files):\n",
        "                    print(f\"  Processed {i + 1}/{len(test_reverb_files)} test files\")\n",
        "\n",
        "            print(f\"\\n‚úÖ Test dataset processed successfully!\")\n",
        "            print(f\"  Total test files: {len(test_reverb_features_list)}\")\n",
        "            print(f\"  Input shape (each): {test_reverb_features_list[0].shape} (normalized)\")\n",
        "            print(f\"  Note: Outputs will be generated after model training\")\n",
        "\n",
        "        print(f\"\\n\" + \"=\"*60)\n",
        "        print(\"PROCESSING COMPLETE\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"‚úì Normalization stats computed from reverberant training audio\")\n",
        "        print(\"‚úì Training pairs created:\")\n",
        "        print(\"  - Input: Normalized reverberant features\")\n",
        "        print(\"  - Target: Unnormalized clean features\")\n",
        "        if has_test_data:\n",
        "            print(\"‚úì Test data processed (ready for inference after training)\")\n",
        "            print(f\"  - Test outputs will be saved to: {test_folder}/output/\")\n",
        "        print(\"‚úì Ready for Step 3 (Model Training)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in processing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class LSTMDereverberation(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM-based Speech Dereverberation Model.\n",
        "    Implements Step 3 from the paper.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_size=257,\n",
        "                 hidden_size=512,\n",
        "                 num_layers=2,\n",
        "                 dropout=0.3,\n",
        "                 weight_dropout=0.5):\n",
        "        super(LSTMDereverberation, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.weight_dropout = weight_dropout\n",
        "\n",
        "        # Build LSTM layers with weight dropout\n",
        "        self.lstm_layers = nn.ModuleList()\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            layer_input_size = input_size if i == 0 else hidden_size\n",
        "\n",
        "            # Create LSTM layer\n",
        "            lstm = nn.LSTM(\n",
        "                input_size=layer_input_size,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers=1,\n",
        "                batch_first=True,\n",
        "                dropout=0\n",
        "            )\n",
        "\n",
        "            # Apply weight dropout to recurrent connections\n",
        "            lstm = WeightDropLSTM(lstm, dropout=weight_dropout)\n",
        "\n",
        "            self.lstm_layers.append(lstm)\n",
        "\n",
        "        # Dropout between LSTM layers\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "        # Linear projection layer to map hidden state to magnitude spectrum\n",
        "        self.linear = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "        # ReLU activation to ensure positive output\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Initialize weights with orthogonal initialization\n",
        "        self._init_weights()\n",
        "\n",
        "        print(f\"Initialized LSTMDereverberation model:\")\n",
        "        print(f\"  Input size: {input_size}\")\n",
        "        print(f\"  Hidden size: {hidden_size}\")\n",
        "        print(f\"  Num LSTM layers: {num_layers}\")\n",
        "        print(f\"  Dropout (between layers): {dropout}\")\n",
        "        print(f\"  Weight dropout (recurrent): {weight_dropout}\")\n",
        "        print(f\"  Total parameters: {self.count_parameters():,}\")\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Orthogonal initialization as per paper\"\"\"\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight_hh_raw' in name:  # WeightDropLSTM recurrent weights\n",
        "                nn.init.orthogonal_(param)\n",
        "            elif 'weight_ih' in name:  # LSTM input weights\n",
        "                nn.init.orthogonal_(param)\n",
        "            elif 'weight' in name and param.dim() >= 2:  # Linear layer weights\n",
        "                nn.init.orthogonal_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "\n",
        "    def forward(self, x, hidden_states=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the LSTM dereverberation model.\n",
        "\n",
        "        Args:\n",
        "            x: Input features (batch_size, seq_len, input_size)\n",
        "            hidden_states: Optional list of (h, c) tuples for each LSTM layer\n",
        "\n",
        "        Returns:\n",
        "            output: Enhanced features (batch_size, seq_len, input_size)\n",
        "            new_hidden_states: Updated hidden states for each layer\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Initialize hidden states if not provided\n",
        "        if hidden_states is None:\n",
        "            hidden_states = self._init_hidden(batch_size, x.device)\n",
        "\n",
        "        # Pass through LSTM layers\n",
        "        lstm_out = x\n",
        "        new_hidden_states = []\n",
        "\n",
        "        for i, lstm in enumerate(self.lstm_layers):\n",
        "            # LSTM forward pass\n",
        "            lstm_out, (h, c) = lstm(lstm_out, hidden_states[i])\n",
        "            new_hidden_states.append((h, c))\n",
        "\n",
        "            # Apply dropout between layers (not after last layer)\n",
        "            if i < self.num_layers - 1:\n",
        "                lstm_out = self.dropout_layer(lstm_out)\n",
        "\n",
        "        # Linear projection to magnitude spectrum\n",
        "        projected = self.linear(lstm_out)\n",
        "\n",
        "        # Apply ReLU to ensure positive magnitude estimates\n",
        "        output = self.relu(projected)\n",
        "\n",
        "        return output, new_hidden_states\n",
        "\n",
        "    def _init_hidden(self, batch_size, device):\n",
        "        \"\"\"Initialize hidden states for all LSTM layers\"\"\"\n",
        "        hidden_states = []\n",
        "        for _ in range(self.num_layers):\n",
        "            h_0 = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
        "            c_0 = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
        "            hidden_states.append((h_0, c_0))\n",
        "        return hidden_states\n",
        "\n",
        "    def count_parameters(self):\n",
        "        \"\"\"Count total trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def predict(self, reverb_features, return_hidden=False):\n",
        "        \"\"\"\n",
        "        Inference mode prediction.\n",
        "\n",
        "        Args:\n",
        "            reverb_features: Reverberant features (seq_len, input_size) or (batch_size, seq_len, input_size)\n",
        "            return_hidden: Whether to return hidden states\n",
        "\n",
        "        Returns:\n",
        "            enhanced_features: Enhanced/denoised features\n",
        "            hidden_states: (optional) Hidden states if return_hidden=True\n",
        "        \"\"\"\n",
        "        # Handle single sequence input\n",
        "        if reverb_features.dim() == 2:\n",
        "            reverb_features = reverb_features.unsqueeze(0)\n",
        "\n",
        "        # Forward pass\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            enhanced_features, hidden_states = self.forward(reverb_features)\n",
        "\n",
        "        if return_hidden:\n",
        "            return enhanced_features, hidden_states\n",
        "        else:\n",
        "            return enhanced_features\n",
        "\n",
        "\n",
        "class WeightDropLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM with weight dropout applied to recurrent connections.\n",
        "    Implements variational dropout on the hidden-to-hidden weights.\n",
        "    \"\"\"\n",
        "    def __init__(self, lstm, dropout=0.5):\n",
        "        super(WeightDropLSTM, self).__init__()\n",
        "        self.lstm = lstm\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Store the original recurrent weight\n",
        "        # We need to save it and remove it from the LSTM's parameters\n",
        "        w_hh = self.lstm.weight_hh_l0.data.clone()\n",
        "        self.weight_hh_raw = nn.Parameter(w_hh)\n",
        "\n",
        "        # Remove the original weight from LSTM\n",
        "        # This prevents it from being optimized separately\n",
        "        del self.lstm._parameters['weight_hh_l0']\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        Forward pass with weight dropout applied.\n",
        "\n",
        "        During training: Apply dropout mask to recurrent weights\n",
        "        During evaluation: Use full weights without dropout\n",
        "        \"\"\"\n",
        "        # Apply dropout to recurrent weights during training\n",
        "        if self.training and self.dropout > 0:\n",
        "            # Create dropout mask (same mask for all timesteps - variational dropout)\n",
        "            mask = self.weight_hh_raw.new_ones(self.weight_hh_raw.size()).bernoulli_(1 - self.dropout)\n",
        "            # Scale by (1 - dropout) to maintain expected value\n",
        "            w_hh = self.weight_hh_raw * mask / (1 - self.dropout)\n",
        "        else:\n",
        "            w_hh = self.weight_hh_raw\n",
        "\n",
        "        # Temporarily assign the weight to LSTM\n",
        "        self.lstm.weight_hh_l0 = w_hh\n",
        "\n",
        "        # Run LSTM forward pass\n",
        "        output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "class DereverberationLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Loss function for dereverberation training.\n",
        "    Uses MSE loss in the cubic-root compressed magnitude domain.\n",
        "    \"\"\"\n",
        "    def __init__(self, loss_type='mse'):\n",
        "        super(DereverberationLoss, self).__init__()\n",
        "        self.loss_type = loss_type\n",
        "        if loss_type == 'mse':\n",
        "            self.criterion = nn.MSELoss()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss type: {loss_type}\")\n",
        "\n",
        "    def forward(self, predicted, target):\n",
        "        \"\"\"\n",
        "        Compute loss between predicted and target features.\n",
        "\n",
        "        Args:\n",
        "            predicted: Model output (batch_size, seq_len, input_size)\n",
        "            target: Target clean features (batch_size, seq_len, input_size)\n",
        "\n",
        "        Returns:\n",
        "            loss: Scalar loss value\n",
        "        \"\"\"\n",
        "        return self.criterion(predicted, target)\n",
        "\n",
        "\n",
        "# === STEP 3 EXECUTION - USING DATA FROM STEP 2 ===\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"LSTM Dereverberation Model - Step 3\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Initialize model\n",
        "    model = LSTMDereverberation(\n",
        "        input_size=257,\n",
        "        hidden_size=512,\n",
        "        num_layers=2,\n",
        "        dropout=0.3,\n",
        "        weight_dropout=0.5\n",
        "    )\n",
        "\n",
        "    print(\"\\nModel Architecture:\")\n",
        "    print(model)\n",
        "\n",
        "    # Test the model with the data from Step 2\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Testing with Step 2 data...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Convert the reverberant features from Step 2 to PyTorch tensor\n",
        "    if 'reverb_features' in locals():\n",
        "        # Use the actual data from Step 2\n",
        "        input_features = torch.from_numpy(reverb_features).float().unsqueeze(0)\n",
        "        target_features_tensor = torch.from_numpy(target_features).float().unsqueeze(0)\n",
        "\n",
        "        print(f\"Input shape from Step 2: {input_features.shape}\")\n",
        "        print(f\"Target shape from Step 2: {target_features_tensor.shape}\")\n",
        "\n",
        "        # Forward pass with real data\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output, hidden_states = model(input_features)\n",
        "\n",
        "        print(f\"Model output shape: {output.shape}\")\n",
        "\n",
        "        # Test loss with real data\n",
        "        criterion = DereverberationLoss(loss_type='mse')\n",
        "        loss = criterion(output, target_features_tensor)\n",
        "        print(f\"MSE Loss with real data: {loss.item():.4f}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 3 SUMMARY:\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"‚úì LSTM model initialized with 2 layers, 512 units each\")\n",
        "        print(\"‚úì Weight dropout (0.5) applied to recurrent connections\")\n",
        "        print(\"‚úì Dropout (0.3) between LSTM layers\")\n",
        "        print(\"‚úì Linear projection + ReLU for positive output\")\n",
        "        print(\"‚úì Orthogonal weight initialization applied\")\n",
        "        print(\"‚úì MSE loss function defined for training\")\n",
        "        print(\"‚úì Successfully tested with Step 2 data\")\n",
        "        print(f\"‚úì Input: normalized reverb features {input_features.shape}\")\n",
        "        print(f\"‚úì Target: unnormalized clean features {target_features_tensor.shape}\")\n",
        "        print(\"‚úì Model ready for training in Step 4!\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    else:\n",
        "        # Fallback: test with dummy data (if Step 2 variables aren't available)\n",
        "        print(\"‚ö†Ô∏è  Step 2 variables not found. Using dummy data for testing...\")\n",
        "        batch_size = 1\n",
        "        seq_len = 100\n",
        "        input_size = 257\n",
        "\n",
        "        dummy_input = torch.randn(batch_size, seq_len, input_size)\n",
        "        print(f\"Dummy input shape: {dummy_input.shape}\")\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output, hidden_states = model(dummy_input)\n",
        "\n",
        "        print(f\"Model output shape: {output.shape}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 3 SUMMARY:\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"‚úì LSTM model initialized with 2 layers, 512 units each\")\n",
        "        print(\"‚úì Weight dropout (0.5) applied to recurrent connections\")\n",
        "        print(\"‚úì Dropout (0.3) between LSTM layers\")\n",
        "        print(\"‚úì Linear projection + ReLU for positive output\")\n",
        "        print(\"‚úì Orthogonal weight initialization applied\")\n",
        "        print(\"‚úì MSE loss function defined for training\")\n",
        "        print(\"‚úì Model tested with dummy data\")\n",
        "        print(\"‚ö†Ô∏è  Run Step 2 first to test with real data\")\n",
        "        print(\"‚úì Model ready for training in Step 4!\")\n",
        "        print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOCEkqIpWt1R",
        "outputId": "cda4f7ef-f306-47cf-dd70-8f0f3ca202f6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "LSTM Dereverberation Model - Step 3\n",
            "============================================================\n",
            "Initialized LSTMDereverberation model:\n",
            "  Input size: 257\n",
            "  Hidden size: 512\n",
            "  Num LSTM layers: 2\n",
            "  Dropout (between layers): 0.3\n",
            "  Weight dropout (recurrent): 0.5\n",
            "  Total parameters: 3,812,097\n",
            "\n",
            "Model Architecture:\n",
            "LSTMDereverberation(\n",
            "  (lstm_layers): ModuleList(\n",
            "    (0): WeightDropLSTM(\n",
            "      (lstm): LSTM(257, 512, batch_first=True)\n",
            "    )\n",
            "    (1): WeightDropLSTM(\n",
            "      (lstm): LSTM(512, 512, batch_first=True)\n",
            "    )\n",
            "  )\n",
            "  (dropout_layer): Dropout(p=0.3, inplace=False)\n",
            "  (linear): Linear(in_features=512, out_features=257, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "\n",
            "============================================================\n",
            "Testing with Step 2 data...\n",
            "============================================================\n",
            "Input shape from Step 2: torch.Size([1, 751, 257])\n",
            "Target shape from Step 2: torch.Size([1, 751, 257])\n",
            "Model output shape: torch.Size([1, 751, 257])\n",
            "MSE Loss with real data: 0.1943\n",
            "\n",
            "============================================================\n",
            "STEP 3 SUMMARY:\n",
            "============================================================\n",
            "‚úì LSTM model initialized with 2 layers, 512 units each\n",
            "‚úì Weight dropout (0.5) applied to recurrent connections\n",
            "‚úì Dropout (0.3) between LSTM layers\n",
            "‚úì Linear projection + ReLU for positive output\n",
            "‚úì Orthogonal weight initialization applied\n",
            "‚úì MSE loss function defined for training\n",
            "‚úì Successfully tested with Step 2 data\n",
            "‚úì Input: normalized reverb features torch.Size([1, 751, 257])\n",
            "‚úì Target: unnormalized clean features torch.Size([1, 751, 257])\n",
            "‚úì Model ready for training in Step 4!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# === STEP 4 CLASS DEFINITIONS ===\n",
        "\n",
        "class DereverberationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for speech dereverberation training with clean+reverb pairs.\n",
        "\n",
        "    Each sample contains:\n",
        "    - reverb_features: Normalized, cubic-root compressed magnitude spectrum (reverberant)\n",
        "    - target_features: Cubic-root compressed magnitude spectrum (clean, NOT normalized)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, reverb_features_list, target_features_list):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            reverb_features_list: List of numpy arrays, each shape (seq_len, 257)\n",
        "            target_features_list: List of numpy arrays, each shape (seq_len, 257)\n",
        "        \"\"\"\n",
        "        assert len(reverb_features_list) == len(target_features_list), \\\n",
        "            f\"Mismatch: {len(reverb_features_list)} reverb vs {len(target_features_list)} target files\"\n",
        "\n",
        "        # Convert to torch tensors if needed\n",
        "        self.reverb_features = []\n",
        "        self.target_features = []\n",
        "\n",
        "        for rev_feat, tgt_feat in zip(reverb_features_list, target_features_list):\n",
        "            if isinstance(rev_feat, np.ndarray):\n",
        "                rev_feat = torch.from_numpy(rev_feat).float()\n",
        "            if isinstance(tgt_feat, np.ndarray):\n",
        "                tgt_feat = torch.from_numpy(tgt_feat).float()\n",
        "\n",
        "            self.reverb_features.append(rev_feat)\n",
        "            self.target_features.append(tgt_feat)\n",
        "\n",
        "        self.num_samples = len(self.reverb_features)\n",
        "\n",
        "        print(f\"Dataset initialized with {self.num_samples} samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            reverb: (seq_len, 257) - normalized, cubic-root compressed (reverberant)\n",
        "            target: (seq_len, 257) - cubic-root compressed (clean, NOT normalized)\n",
        "            seq_len: scalar - length of sequence\n",
        "        \"\"\"\n",
        "        reverb = self.reverb_features[idx]\n",
        "        target = self.target_features[idx]\n",
        "        seq_len = reverb.shape[0]\n",
        "\n",
        "        return reverb, target, seq_len\n",
        "\n",
        "\n",
        "def collate_fn_variable_length(batch):\n",
        "    \"\"\"\n",
        "    Collate function for variable-length sequences.\n",
        "\n",
        "    Args:\n",
        "        batch: List of tuples (reverb, target, seq_len)\n",
        "\n",
        "    Returns:\n",
        "        reverb_padded: (batch_size, max_seq_len, 257)\n",
        "        target_padded: (batch_size, max_seq_len, 257)\n",
        "        lengths: (batch_size,) - actual lengths before padding\n",
        "    \"\"\"\n",
        "    reverb_list = [item[0] for item in batch]\n",
        "    target_list = [item[1] for item in batch]\n",
        "    lengths = torch.tensor([item[2] for item in batch])\n",
        "\n",
        "    # Pad sequences to max length in batch\n",
        "    reverb_padded = pad_sequence(reverb_list, batch_first=True, padding_value=0.0)\n",
        "    target_padded = pad_sequence(target_list, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    return reverb_padded, target_padded, lengths\n",
        "\n",
        "\n",
        "class DereverberationTrainer:\n",
        "    \"\"\"\n",
        "    Trainer for LSTM-based dereverberation model.\n",
        "    Implements Step 4 from the paper.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 train_dataset,\n",
        "                 val_dataset=None,\n",
        "                 batch_size=8,\n",
        "                 learning_rate=0.001,\n",
        "                 device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "                 checkpoint_dir=None):  # Make checkpoint_dir optional\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model: LSTMDereverberation model (from Step 3)\n",
        "            train_dataset: Training dataset (DereverberationDataset)\n",
        "            val_dataset: Validation dataset (DereverberationDataset) - optional\n",
        "            batch_size: Batch size (8 as per paper)\n",
        "            learning_rate: Learning rate for Adam optimizer\n",
        "            device: Device to train on\n",
        "            checkpoint_dir: Directory to save checkpoints (Google Drive path)\n",
        "        \"\"\"\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Set checkpoint directory to your existing dataset folder\n",
        "        if checkpoint_dir is None:\n",
        "            # Save to your existing Serveroperation_dataset folder\n",
        "            try:\n",
        "                from google.colab import drive\n",
        "                drive.mount('/content/drive')\n",
        "                self.checkpoint_dir = '/content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints'\n",
        "            except:\n",
        "                self.checkpoint_dir = './dereverberation_checkpoints'\n",
        "        else:\n",
        "            self.checkpoint_dir = checkpoint_dir\n",
        "\n",
        "        print(\"Model weights already initialized with orthogonal initialization (from Step 3)\")\n",
        "\n",
        "        # Create training data loader\n",
        "        self.train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_fn_variable_length,\n",
        "            num_workers=0,\n",
        "            pin_memory=True if device == 'cuda' else False\n",
        "        )\n",
        "\n",
        "        # Create validation data loader if provided\n",
        "        self.has_val = val_dataset is not None and len(val_dataset) > 0\n",
        "        if self.has_val:\n",
        "            self.val_loader = DataLoader(\n",
        "                val_dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False,\n",
        "                collate_fn=collate_fn_variable_length,\n",
        "                num_workers=0,\n",
        "                pin_memory=True if device == 'cuda' else False\n",
        "            )\n",
        "        else:\n",
        "            self.val_loader = None\n",
        "            print(\"‚ö†Ô∏è  No validation dataset provided - will only track training loss\")\n",
        "\n",
        "        # Loss function: MSE (as per paper)\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "        # Optimizer: Adam (as per paper)\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate\n",
        "        )\n",
        "\n",
        "        # Create checkpoint directory\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "        # Training history\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.best_val_loss = float('inf')\n",
        "\n",
        "        print(f\"\\nTrainer initialized:\")\n",
        "        print(f\"  Device: {device}\")\n",
        "        print(f\"  Batch size: {batch_size}\")\n",
        "        print(f\"  Learning rate: {learning_rate}\")\n",
        "        print(f\"  Training samples: {len(train_dataset)}\")\n",
        "        if self.has_val:\n",
        "            print(f\"  Validation samples: {len(val_dataset)}\")\n",
        "        print(f\"  Training batches per epoch: {len(self.train_loader)}\")\n",
        "        if self.has_val:\n",
        "            print(f\"  Validation batches per epoch: {len(self.val_loader)}\")\n",
        "        print(f\"  Checkpoint directory: {self.checkpoint_dir}\")\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "        \"\"\"\n",
        "        Train for one epoch.\n",
        "\n",
        "        Args:\n",
        "            epoch: Current epoch number\n",
        "\n",
        "        Returns:\n",
        "            avg_loss: Average training loss for the epoch\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch} [Train]\")\n",
        "\n",
        "        for batch_idx, (reverb, target, lengths) in enumerate(pbar):\n",
        "            # Move to device\n",
        "            reverb = reverb.to(self.device)\n",
        "            target = target.to(self.device)\n",
        "            lengths = lengths.to(self.device)\n",
        "\n",
        "            # Zero gradients\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            predicted, _ = self.model(reverb)\n",
        "\n",
        "            # Compute loss (MSE in cubic root compressed space)\n",
        "            loss = self.compute_loss_with_masking(predicted, target, lengths)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping (recommended for RNNs)\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n",
        "\n",
        "            # Update weights\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Accumulate loss\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        return avg_loss\n",
        "\n",
        "    def validate(self, epoch):\n",
        "        \"\"\"\n",
        "        Validate the model.\n",
        "\n",
        "        Args:\n",
        "            epoch: Current epoch number\n",
        "\n",
        "        Returns:\n",
        "            avg_loss: Average validation loss (or None if no validation set)\n",
        "        \"\"\"\n",
        "        if not self.has_val:\n",
        "            return None\n",
        "\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        pbar = tqdm(self.val_loader, desc=f\"Epoch {epoch} [Val]\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for reverb, target, lengths in pbar:\n",
        "                # Move to device\n",
        "                reverb = reverb.to(self.device)\n",
        "                target = target.to(self.device)\n",
        "                lengths = lengths.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                predicted, _ = self.model(reverb)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.compute_loss_with_masking(predicted, target, lengths)\n",
        "\n",
        "                # Accumulate loss\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "                # Update progress bar\n",
        "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        return avg_loss\n",
        "\n",
        "    def compute_loss_with_masking(self, predicted, target, lengths):\n",
        "        \"\"\"\n",
        "        Compute MSE loss only on non-padded frames.\n",
        "\n",
        "        Args:\n",
        "            predicted: (batch_size, max_seq_len, 257)\n",
        "            target: (batch_size, max_seq_len, 257)\n",
        "            lengths: (batch_size,) - actual sequence lengths\n",
        "\n",
        "        Returns:\n",
        "            loss: Scalar loss value\n",
        "        \"\"\"\n",
        "        batch_size, max_seq_len, feat_dim = predicted.shape\n",
        "\n",
        "        # Create mask for non-padded frames\n",
        "        mask = torch.arange(max_seq_len, device=self.device).unsqueeze(0) < lengths.unsqueeze(1)\n",
        "        mask = mask.unsqueeze(-1).expand(-1, -1, feat_dim)  # (batch_size, max_seq_len, 257)\n",
        "\n",
        "        # Apply mask\n",
        "        predicted_masked = predicted * mask\n",
        "        target_masked = target * mask\n",
        "\n",
        "        # Compute MSE loss\n",
        "        squared_diff = (predicted_masked - target_masked) ** 2\n",
        "\n",
        "        # Average over non-padded elements only\n",
        "        total_elements = mask.sum()\n",
        "        loss = squared_diff.sum() / total_elements\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train(self, num_epochs, save_every=5):\n",
        "        \"\"\"\n",
        "        Train the model for multiple epochs.\n",
        "\n",
        "        Args:\n",
        "            num_epochs: Number of epochs to train\n",
        "            save_every: Save checkpoint every N epochs\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Starting training for {num_epochs} epochs\")\n",
        "        print(f\"Checkpoints will be saved to: {self.checkpoint_dir}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "            # Train\n",
        "            train_loss = self.train_epoch(epoch)\n",
        "            self.train_losses.append(train_loss)\n",
        "\n",
        "            # Validate (if validation set exists)\n",
        "            val_loss = self.validate(epoch) if self.has_val else None\n",
        "            if val_loss is not None:\n",
        "                self.val_losses.append(val_loss)\n",
        "\n",
        "            # Print epoch summary\n",
        "            print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
        "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "            if val_loss is not None:\n",
        "                print(f\"  Val Loss:   {val_loss:.4f}\")\n",
        "\n",
        "            # Save best model based on validation loss (or training loss if no validation)\n",
        "            loss_to_compare = val_loss if val_loss is not None else train_loss\n",
        "            if loss_to_compare < self.best_val_loss:\n",
        "                self.best_val_loss = loss_to_compare\n",
        "                self.save_checkpoint(epoch, is_best=True)\n",
        "                metric_name = \"Val\" if val_loss is not None else \"Train\"\n",
        "                print(f\"  ‚úì New best model saved! ({metric_name} Loss: {loss_to_compare:.4f})\")\n",
        "\n",
        "            # Save periodic checkpoint\n",
        "            if epoch % save_every == 0:\n",
        "                self.save_checkpoint(epoch, is_best=False)\n",
        "                print(f\"  ‚úì Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "            print()\n",
        "\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Training completed!\")\n",
        "        metric_name = \"validation\" if self.has_val else \"training\"\n",
        "        print(f\"Best {metric_name} loss: {self.best_val_loss:.4f}\")\n",
        "        print(f\"All models saved to: {self.checkpoint_dir}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "    def save_checkpoint(self, epoch, is_best=False):\n",
        "        \"\"\"\n",
        "        Save model checkpoint to Google Drive.\n",
        "\n",
        "        Args:\n",
        "            epoch: Current epoch\n",
        "            is_best: Whether this is the best model so far\n",
        "        \"\"\"\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'train_losses': self.train_losses,\n",
        "            'val_losses': self.val_losses,\n",
        "            'best_val_loss': self.best_val_loss\n",
        "        }\n",
        "\n",
        "        if is_best:\n",
        "            path = os.path.join(self.checkpoint_dir, 'best_model.pt')\n",
        "        else:\n",
        "            path = os.path.join(self.checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
        "\n",
        "        torch.save(checkpoint, path)\n",
        "        print(f\"  Checkpoint saved: {path}\")\n",
        "\n",
        "    def load_checkpoint(self, checkpoint_path=None):\n",
        "        \"\"\"\n",
        "        Load model checkpoint from Google Drive.\n",
        "\n",
        "        Args:\n",
        "            checkpoint_path: Path to checkpoint file (if None, loads best_model.pt)\n",
        "        \"\"\"\n",
        "        if checkpoint_path is None:\n",
        "            checkpoint_path = os.path.join(self.checkpoint_dir, 'best_model.pt')\n",
        "\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
        "\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.train_losses = checkpoint['train_losses']\n",
        "        self.val_losses = checkpoint['val_losses']\n",
        "        self.best_val_loss = checkpoint['best_val_loss']\n",
        "\n",
        "        print(f\"Checkpoint loaded from {checkpoint_path}\")\n",
        "        print(f\"Epoch: {checkpoint['epoch']}\")\n",
        "        print(f\"Best val loss: {self.best_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "# === STEP 4 EXECUTION - ADAPTIVE FOR ANY NUMBER OF PAIRS ===\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"LSTM Dereverberation Training - Step 4\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Check if we have training data from Step 2\n",
        "    # Step 2 should provide: reverb_features_list and target_features_list\n",
        "    if 'reverb_features_list' in locals() and 'target_features_list' in locals():\n",
        "        num_samples = len(reverb_features_list)\n",
        "        print(f\"‚úì Found {num_samples} training pairs from Step 2\")\n",
        "\n",
        "        # Adaptive splitting strategy based on number of samples\n",
        "        if num_samples == 1:\n",
        "            # Single pair - no validation\n",
        "            print(\"‚ö†Ô∏è  Single audio pair - using for training only (no validation)\")\n",
        "            train_dataset = DereverberationDataset(reverb_features_list, target_features_list)\n",
        "            val_dataset = None\n",
        "            batch_size = 1\n",
        "            num_epochs = 10\n",
        "            print(\"   Model will memorize this single example\")\n",
        "\n",
        "        elif num_samples < 10:\n",
        "            # Few pairs (2-9) - use all for training\n",
        "            print(f\"‚ö†Ô∏è  Only {num_samples} pairs - using all for training (no validation)\")\n",
        "            train_dataset = DereverberationDataset(reverb_features_list, target_features_list)\n",
        "            val_dataset = None\n",
        "            batch_size = min(8, num_samples)\n",
        "            num_epochs = 10\n",
        "            print(f\"   Batch size: {batch_size}\")\n",
        "\n",
        "        else:\n",
        "            # Many pairs (10+) - split train/val 80/20\n",
        "            split_idx = int(0.8 * num_samples)\n",
        "            print(f\"‚úì Splitting {num_samples} pairs: {split_idx} train, {num_samples - split_idx} validation\")\n",
        "\n",
        "            train_dataset = DereverberationDataset(\n",
        "                reverb_features_list[:split_idx],\n",
        "                target_features_list[:split_idx]\n",
        "            )\n",
        "            val_dataset = DereverberationDataset(\n",
        "                reverb_features_list[split_idx:],\n",
        "                target_features_list[split_idx:]\n",
        "            )\n",
        "            batch_size = 8\n",
        "            num_epochs = 10\n",
        "\n",
        "        # Initialize model from Step 3\n",
        "        if 'LSTMDereverberation' not in dir():\n",
        "            print(\"‚ùå LSTMDereverberation class not found!\")\n",
        "            print(\"   Please run Step 3 first or ensure it's in the same script\")\n",
        "            raise ImportError(\"LSTMDereverberation class must be defined before Step 4\")\n",
        "\n",
        "        model = LSTMDereverberation(\n",
        "            input_size=257,\n",
        "            hidden_size=512,\n",
        "            num_layers=2,\n",
        "            dropout=0.3,\n",
        "            weight_dropout=0.5\n",
        "        )\n",
        "\n",
        "        # Set checkpoint path to your existing dataset folder\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            checkpoint_dir = '/content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints'\n",
        "            print(f\"‚úì Google Drive mounted - checkpoints will save to: {checkpoint_dir}\")\n",
        "        except:\n",
        "            checkpoint_dir = './dereverberation_checkpoints'\n",
        "            print(f\"‚ö†Ô∏è  Running locally - checkpoints will save to: {checkpoint_dir}\")\n",
        "\n",
        "        # Initialize trainer\n",
        "        trainer = DereverberationTrainer(\n",
        "            model=model,\n",
        "            train_dataset=train_dataset,\n",
        "            val_dataset=val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            learning_rate=0.001,\n",
        "            checkpoint_dir=checkpoint_dir\n",
        "        )\n",
        "\n",
        "        print(f\"\\nTraining configuration:\")\n",
        "        print(f\"  Total samples: {num_samples}\")\n",
        "        print(f\"  Training samples: {len(train_dataset)}\")\n",
        "        if val_dataset:\n",
        "            print(f\"  Validation samples: {len(val_dataset)}\")\n",
        "        print(f\"  Batch size: {batch_size}\")\n",
        "        print(f\"  Epochs: {num_epochs}\")\n",
        "        print(f\"  Learning rate: 0.001\")\n",
        "        print(f\"  Checkpoint location: {checkpoint_dir}\")\n",
        "        print(f\"  Training objective: normalized reverb ‚Üí unnormalized clean\")\n",
        "\n",
        "        # Start training automatically\n",
        "        print(\"\\nStarting training...\")\n",
        "        trainer.train(num_epochs=num_epochs, save_every=10)\n",
        "\n",
        "        # ============================================================\n",
        "        # POST-TRAINING EVALUATION\n",
        "        # ============================================================\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"POST-TRAINING EVALUATION\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Load the best model for evaluation\n",
        "        best_model_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
        "        trainer.load_checkpoint(best_model_path)\n",
        "\n",
        "        # Evaluate on training set (or validation if available)\n",
        "        print(\"\\nEvaluating model performance...\")\n",
        "        trainer.model.eval()\n",
        "\n",
        "        eval_dataset = val_dataset if val_dataset else train_dataset\n",
        "        eval_loader = DataLoader(\n",
        "            eval_dataset,\n",
        "            batch_size=1,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn_variable_length\n",
        "        )\n",
        "\n",
        "        total_mse = 0.0\n",
        "        total_mae = 0.0\n",
        "        num_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for reverb, target, lengths in tqdm(eval_loader, desc=\"Evaluating\"):\n",
        "                reverb = reverb.to(trainer.device)\n",
        "                target = target.to(trainer.device)\n",
        "                lengths = lengths.to(trainer.device)\n",
        "\n",
        "                # Forward pass\n",
        "                predicted, _ = trainer.model(reverb)\n",
        "\n",
        "                # Compute metrics (only on non-padded regions)\n",
        "                batch_size, max_seq_len, feat_dim = predicted.shape\n",
        "                mask = torch.arange(max_seq_len, device=trainer.device).unsqueeze(0) < lengths.unsqueeze(1)\n",
        "                mask = mask.unsqueeze(-1).expand(-1, -1, feat_dim)\n",
        "\n",
        "                # MSE\n",
        "                mse = ((predicted - target) ** 2 * mask).sum() / mask.sum()\n",
        "                total_mse += mse.item()\n",
        "\n",
        "                # MAE (Mean Absolute Error)\n",
        "                mae = (torch.abs(predicted - target) * mask).sum() / mask.sum()\n",
        "                total_mae += mae.item()\n",
        "\n",
        "                num_samples += 1\n",
        "\n",
        "        avg_mse = total_mse / num_samples\n",
        "        avg_mae = total_mae / num_samples\n",
        "        rmse = np.sqrt(avg_mse)\n",
        "\n",
        "        # Calculate approximate SNR improvement (rough estimate)\n",
        "        snr_improvement_estimate = -10 * np.log10(avg_mse + 1e-10)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"MODEL PERFORMANCE METRICS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"üìä Evaluation Dataset: {'Validation' if val_dataset else 'Training'} set\")\n",
        "        print(f\"üìà Number of samples evaluated: {num_samples}\")\n",
        "        print(f\"\\nüéØ Reconstruction Metrics (in cubic-root compressed space):\")\n",
        "        print(f\"   ‚Ä¢ Mean Squared Error (MSE):  {avg_mse:.6f}\")\n",
        "        print(f\"   ‚Ä¢ Root Mean Squared Error:   {rmse:.6f}\")\n",
        "        print(f\"   ‚Ä¢ Mean Absolute Error (MAE): {avg_mae:.6f}\")\n",
        "        print(f\"\\nüîä Estimated Quality Improvement:\")\n",
        "        print(f\"   ‚Ä¢ SNR Improvement (approx):  {snr_improvement_estimate:.2f} dB\")\n",
        "\n",
        "        # Interpret the results\n",
        "        print(f\"\\nüí° Interpretation:\")\n",
        "        if avg_mse < 0.01:\n",
        "            print(\"   ‚úÖ EXCELLENT: Very low error - model learned the mapping well!\")\n",
        "        elif avg_mse < 0.05:\n",
        "            print(\"   ‚úÖ GOOD: Reasonable error - model shows learning progress\")\n",
        "        elif avg_mse < 0.1:\n",
        "            print(\"   ‚ö†Ô∏è  FAIR: Moderate error - may need more training or data\")\n",
        "        else:\n",
        "            print(\"   ‚ùå POOR: High error - needs more epochs, data, or architecture tuning\")\n",
        "\n",
        "        if num_samples == 1:\n",
        "            print(\"   ‚ö†Ô∏è  NOTE: Evaluated on single sample - model likely memorized it\")\n",
        "            print(\"      For real assessment, test on unseen audio files!\")\n",
        "        elif not val_dataset:\n",
        "            print(\"   ‚ö†Ô∏è  NOTE: Evaluated on training data - may be overfitting\")\n",
        "            print(\"      For real assessment, use separate validation/test set!\")\n",
        "\n",
        "        # Training history visualization\n",
        "        print(f\"\\nüìâ Training History:\")\n",
        "        print(f\"   ‚Ä¢ Initial training loss: {trainer.train_losses[0]:.6f}\")\n",
        "        print(f\"   ‚Ä¢ Final training loss:   {trainer.train_losses[-1]:.6f}\")\n",
        "        print(f\"   ‚Ä¢ Loss reduction:        {((trainer.train_losses[0] - trainer.train_losses[-1]) / trainer.train_losses[0] * 100):.1f}%\")\n",
        "\n",
        "        if trainer.val_losses:\n",
        "            print(f\"   ‚Ä¢ Best validation loss:  {trainer.best_val_loss:.6f}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 4 COMPLETED:\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"‚úì Model trained on {num_samples} audio pairs\")\n",
        "        print(f\"‚úì Checkpoints saved to: {checkpoint_dir}\")\n",
        "        print(\"‚úì Best model saved as 'best_model.pt'\")\n",
        "        print(\"‚úì Ready for Step 5 (Inference)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    elif 'reverb_features' in locals() and 'target_features' in locals():\n",
        "        # Fallback: single pair from old Step 2 format\n",
        "        print(\"‚úì Found single training pair from Step 2 (old format)\")\n",
        "        print(\"‚ö†Ô∏è  Converting to list format...\")\n",
        "\n",
        "        reverb_features_list = [reverb_features]\n",
        "        target_features_list = [target_features]\n",
        "\n",
        "        train_dataset = DereverberationDataset(reverb_features_list, target_features_list)\n",
        "        val_dataset = None\n",
        "        batch_size = 1\n",
        "        num_epochs = 10\n",
        "\n",
        "        if 'LSTMDereverberation' not in dir():\n",
        "            raise ImportError(\"LSTMDereverberation class must be defined before Step 4\")\n",
        "\n",
        "        # Set checkpoint path to your existing dataset folder\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            checkpoint_dir = '/content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints'\n",
        "            print(f\"‚úì Google Drive mounted - checkpoints will save to: {checkpoint_dir}\")\n",
        "        except:\n",
        "            checkpoint_dir = './dereverberation_checkpoints'\n",
        "            print(f\"‚ö†Ô∏è  Running locally - checkpoints will save to: {checkpoint_dir}\")\n",
        "\n",
        "        model = LSTMDereverberation(\n",
        "            input_size=257,\n",
        "            hidden_size=512,\n",
        "            num_layers=2,\n",
        "            dropout=0.3,\n",
        "            weight_dropout=0.5\n",
        "        )\n",
        "\n",
        "        trainer = DereverberationTrainer(\n",
        "            model=model,\n",
        "            train_dataset=train_dataset,\n",
        "            val_dataset=val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            learning_rate=0.001,\n",
        "            checkpoint_dir=checkpoint_dir\n",
        "        )\n",
        "\n",
        "        print(\"\\nStarting training...\")\n",
        "        trainer.train(num_epochs=num_epochs, save_every=10)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 4 COMPLETED:\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"‚úì Model trained on single audio pair\")\n",
        "        print(f\"‚úì Checkpoints saved to: {checkpoint_dir}\")\n",
        "        print(\"‚úì Best model saved as 'best_model.pt'\")\n",
        "        print(\"‚úì Ready for Step 5 (Inference)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå No training data found from Step 2\")\n",
        "        print(\"Please run Step 2 first to generate training data\")\n",
        "        print(\"\\nExpected variables:\")\n",
        "        print(\"  - reverb_features_list: List of reverberant features\")\n",
        "        print(\"  - target_features_list: List of clean target features\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 4 SUMMARY:\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"‚úì Dataset class for variable-length sequences\")\n",
        "        print(\"‚úì Collate function for padding\")\n",
        "        print(\"‚úì Trainer with MSE loss in cubic root space\")\n",
        "        print(\"‚úì Adam optimizer with gradient clipping\")\n",
        "        print(\"‚úì Checkpoint saving to your dataset folder\")\n",
        "        print(\"‚úì Adaptive splitting: 1 pair ‚Üí no val, 10+ pairs ‚Üí 80/20 split\")\n",
        "        print(\"‚ö†Ô∏è  Waiting for training data from Step 2\")\n",
        "        print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4G2fS4JhGDe",
        "outputId": "8af9f7e0-3528-4d90-97a6-71f68de1e13b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "LSTM Dereverberation Training - Step 4\n",
            "============================================================\n",
            "‚úì Found 2 training pairs from Step 2\n",
            "‚ö†Ô∏è  Only 2 pairs - using all for training (no validation)\n",
            "Dataset initialized with 2 samples\n",
            "   Batch size: 2\n",
            "Initialized LSTMDereverberation model:\n",
            "  Input size: 257\n",
            "  Hidden size: 512\n",
            "  Num LSTM layers: 2\n",
            "  Dropout (between layers): 0.3\n",
            "  Weight dropout (recurrent): 0.5\n",
            "  Total parameters: 3,812,097\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úì Google Drive mounted - checkpoints will save to: /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints\n",
            "Model weights already initialized with orthogonal initialization (from Step 3)\n",
            "‚ö†Ô∏è  No validation dataset provided - will only track training loss\n",
            "\n",
            "Trainer initialized:\n",
            "  Device: cpu\n",
            "  Batch size: 2\n",
            "  Learning rate: 0.001\n",
            "  Training samples: 2\n",
            "  Training batches per epoch: 1\n",
            "  Checkpoint directory: /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints\n",
            "\n",
            "Training configuration:\n",
            "  Total samples: 2\n",
            "  Training samples: 2\n",
            "  Batch size: 2\n",
            "  Epochs: 10\n",
            "  Learning rate: 0.001\n",
            "  Checkpoint location: /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints\n",
            "  Training objective: normalized reverb ‚Üí unnormalized clean\n",
            "\n",
            "Starting training...\n",
            "\n",
            "============================================================\n",
            "Starting training for 10 epochs\n",
            "Checkpoints will be saved to: /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.21s/it, loss=0.2341]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "  Train Loss: 0.2341\n",
            "  Checkpoint saved: /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints/best_model.pt\n",
            "  ‚úì New best model saved! (Train Loss: 0.2341)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.34s/it, loss=0.2168]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/10\n",
            "  Train Loss: 0.2168\n",
            "  Checkpoint saved: /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints/best_model.pt\n",
            "  ‚úì New best model saved! (Train Loss: 0.2168)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.24s/it, loss=0.1904]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/10\n",
            "  Train Loss: 0.1904\n",
            "  Checkpoint saved: /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints/best_model.pt\n",
            "  ‚úì New best model saved! (Train Loss: 0.1904)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.21s/it, loss=0.1708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/10\n",
            "  Train Loss: 0.1708\n",
            "  Checkpoint saved: /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints/best_model.pt\n",
            "  ‚úì New best model saved! (Train Loss: 0.1708)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.29s/it, loss=0.1562]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/10\n",
            "  Train Loss: 0.1562\n",
            "  Checkpoint saved: /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints/best_model.pt\n",
            "  ‚úì New best model saved! (Train Loss: 0.1562)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.21s/it, loss=0.1392]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/10\n",
            "  Train Loss: 0.1392\n",
            "  Checkpoint saved: /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints/best_model.pt\n",
            "  ‚úì New best model saved! (Train Loss: 0.1392)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.70s/it, loss=0.1312]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/10\n",
            "  Train Loss: 0.1312\n",
            "  Checkpoint saved: /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints/best_model.pt\n",
            "  ‚úì New best model saved! (Train Loss: 0.1312)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.71s/it, loss=0.1252]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/10\n",
            "  Train Loss: 0.1252\n",
            "  Checkpoint saved: /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints/best_model.pt\n",
            "  ‚úì New best model saved! (Train Loss: 0.1252)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.38s/it, loss=0.1184]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/10\n",
            "  Train Loss: 0.1184\n",
            "  Checkpoint saved: /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints/best_model.pt\n",
            "  ‚úì New best model saved! (Train Loss: 0.1184)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.22s/it, loss=0.1097]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/10\n",
            "  Train Loss: 0.1097\n",
            "  Checkpoint saved: /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints/best_model.pt\n",
            "  ‚úì New best model saved! (Train Loss: 0.1097)\n",
            "  Checkpoint saved: /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints/checkpoint_epoch_10.pt\n",
            "  ‚úì Checkpoint saved at epoch 10\n",
            "\n",
            "============================================================\n",
            "Training completed!\n",
            "Best training loss: 0.1097\n",
            "All models saved to: /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "POST-TRAINING EVALUATION\n",
            "============================================================\n",
            "Checkpoint loaded from /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints/best_model.pt\n",
            "Epoch: 10\n",
            "Best val loss: 0.1097\n",
            "\n",
            "Evaluating model performance...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "MODEL PERFORMANCE METRICS\n",
            "============================================================\n",
            "üìä Evaluation Dataset: Training set\n",
            "üìà Number of samples evaluated: 2\n",
            "\n",
            "üéØ Reconstruction Metrics (in cubic-root compressed space):\n",
            "   ‚Ä¢ Mean Squared Error (MSE):  0.101044\n",
            "   ‚Ä¢ Root Mean Squared Error:   0.317874\n",
            "   ‚Ä¢ Mean Absolute Error (MAE): 0.214388\n",
            "\n",
            "üîä Estimated Quality Improvement:\n",
            "   ‚Ä¢ SNR Improvement (approx):  9.95 dB\n",
            "\n",
            "üí° Interpretation:\n",
            "   ‚ùå POOR: High error - needs more epochs, data, or architecture tuning\n",
            "   ‚ö†Ô∏è  NOTE: Evaluated on training data - may be overfitting\n",
            "      For real assessment, use separate validation/test set!\n",
            "\n",
            "üìâ Training History:\n",
            "   ‚Ä¢ Initial training loss: 0.234087\n",
            "   ‚Ä¢ Final training loss:   0.109708\n",
            "   ‚Ä¢ Loss reduction:        53.1%\n",
            "\n",
            "============================================================\n",
            "STEP 4 COMPLETED:\n",
            "============================================================\n",
            "‚úì Model trained on 2 audio pairs\n",
            "‚úì Checkpoints saved to: /content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints\n",
            "‚úì Best model saved as 'best_model.pt'\n",
            "‚úì Ready for Step 5 (Inference)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import os\n",
        "\n",
        "class DereverberationInference:\n",
        "    \"\"\"\n",
        "    Inference pipeline for LSTM-based speech dereverberation.\n",
        "    Implements Steps 5 & 6 from the paper.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model_checkpoint_path,\n",
        "                 preprocessor,\n",
        "                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        \"\"\"\n",
        "        Initialize inference pipeline.\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "        # Verify preprocessor has normalization statistics\n",
        "        if self.preprocessor.feature_mean is None or self.preprocessor.feature_std is None:\n",
        "            raise ValueError(\n",
        "                \"Preprocessor must have normalization statistics computed.\"\n",
        "            )\n",
        "\n",
        "        # Load trained model\n",
        "        print(f\"Loading model from: {model_checkpoint_path}\")\n",
        "        self.model = self._load_model(model_checkpoint_path)\n",
        "        self.model.eval()\n",
        "\n",
        "        print(f\"Inference pipeline initialized on device: {device}\")\n",
        "\n",
        "    def _load_model(self, checkpoint_path):\n",
        "        \"\"\"\n",
        "        Load trained model from checkpoint.\n",
        "        \"\"\"\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
        "\n",
        "        # Reconstruct model architecture\n",
        "        model = LSTMDereverberation(\n",
        "            input_size=257,\n",
        "            hidden_size=512,\n",
        "            num_layers=2,\n",
        "            dropout=0.3,\n",
        "            weight_dropout=0.5\n",
        "        )\n",
        "\n",
        "        # Load trained weights\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model.to(self.device)\n",
        "\n",
        "        print(f\"Model loaded successfully!\")\n",
        "        print(f\"  Trained for {checkpoint['epoch']} epochs\")\n",
        "        print(f\"  Best validation loss: {checkpoint['best_val_loss']:.4f}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def enhance_audio(self, audio_input):\n",
        "        \"\"\"\n",
        "        Complete enhancement pipeline: Steps 5 & 6.\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 5: Enhancement Process\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Load audio if file path provided\n",
        "        if isinstance(audio_input, str):\n",
        "            print(f\"Loading audio from: {audio_input}\")\n",
        "            audio, sr = librosa.load(audio_input, sr=self.preprocessor.sample_rate, mono=True)\n",
        "        else:\n",
        "            audio = audio_input\n",
        "\n",
        "        print(f\"Audio length: {len(audio)} samples ({len(audio)/self.preprocessor.sample_rate:.2f} seconds)\")\n",
        "\n",
        "        # Step 5.1: Extract magnitude spectrum and phase from reverberant audio\n",
        "        print(\"Extracting magnitude spectrum and phase...\")\n",
        "        magnitude_reverb, phase_reverb = self.preprocessor.extract_magnitude_spectrum(audio)\n",
        "\n",
        "        # Step 5.2: Apply cubic root compression\n",
        "        print(\"Applying cubic root compression...\")\n",
        "        compressed_reverb = self.preprocessor.apply_cubic_root_compression(magnitude_reverb)\n",
        "\n",
        "        # Step 5.3: Normalize using training statistics\n",
        "        print(\"Normalizing features using training statistics...\")\n",
        "        normalized_reverb = self.preprocessor.normalize_features(\n",
        "            compressed_reverb,\n",
        "            compute_stats=False\n",
        "        )\n",
        "\n",
        "        # Step 5.4: Prepare input for LSTM\n",
        "        print(\"Preparing input for LSTM...\")\n",
        "        input_tensor = torch.from_numpy(normalized_reverb).float()\n",
        "        input_tensor = input_tensor.unsqueeze(0)  # (1, seq_len, 257)\n",
        "        input_tensor = input_tensor.to(self.device)\n",
        "\n",
        "        # Step 5.5: Forward pass through LSTM\n",
        "        print(\"Running LSTM inference...\")\n",
        "        with torch.no_grad():\n",
        "            enhanced_normalized, _ = self.model(input_tensor)\n",
        "\n",
        "        # Move back to CPU and remove batch dimension\n",
        "        enhanced_normalized = enhanced_normalized.squeeze(0).cpu().numpy()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 6: Signal Reconstruction\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Step 6.1: Denormalize the LSTM output\n",
        "        print(\"Denormalizing LSTM output...\")\n",
        "        enhanced_compressed = self._denormalize(enhanced_normalized)\n",
        "\n",
        "        # Step 6.2: Reverse cubic root compression (cube it)\n",
        "        print(\"Reversing cubic root compression (cubing)...\")\n",
        "        enhanced_magnitude = np.power(enhanced_compressed, 3.0)\n",
        "\n",
        "        # Step 6.3: Combine enhanced magnitude with original reverberant phase\n",
        "        print(\"Combining with original phase...\")\n",
        "        enhanced_complex = enhanced_magnitude * np.exp(1j * phase_reverb)\n",
        "\n",
        "        # Step 6.4: Reconstruct time-domain signal via inverse STFT\n",
        "        print(\"Reconstructing time-domain signal (inverse STFT)...\")\n",
        "        enhanced_audio = self._inverse_stft(enhanced_complex.T)  # Transpose for librosa\n",
        "\n",
        "        # Normalize to prevent clipping\n",
        "        max_val = np.max(np.abs(enhanced_audio))\n",
        "        if max_val > 0:\n",
        "            enhanced_audio = enhanced_audio / max_val * 0.95\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"Enhancement complete!\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return enhanced_audio\n",
        "\n",
        "    def _denormalize(self, normalized_features):\n",
        "        \"\"\"\n",
        "        Denormalize features using training statistics.\n",
        "        \"\"\"\n",
        "        mean = self.preprocessor.feature_mean\n",
        "        std = self.preprocessor.feature_std\n",
        "\n",
        "        denormalized = normalized_features * std + mean\n",
        "        return denormalized\n",
        "\n",
        "    def _inverse_stft(self, complex_spectrum):\n",
        "        \"\"\"\n",
        "        Reconstruct time-domain signal from complex spectrum.\n",
        "        \"\"\"\n",
        "        audio = librosa.istft(\n",
        "            complex_spectrum,\n",
        "            hop_length=self.preprocessor.hop_length,\n",
        "            win_length=self.preprocessor.frame_length,\n",
        "            window='hamming',\n",
        "            center=True\n",
        "        )\n",
        "        return audio\n",
        "\n",
        "    def save_audio(self, audio, output_path, sample_rate=None):\n",
        "        \"\"\"\n",
        "        Save audio to WAV file.\n",
        "        \"\"\"\n",
        "        if sample_rate is None:\n",
        "            sample_rate = self.preprocessor.sample_rate\n",
        "\n",
        "        sf.write(output_path, audio, sample_rate)\n",
        "        print(f\"Audio saved to: {output_path}\")\n",
        "\n",
        "\n",
        "# === STEP 5 & 6 EXECUTION ===\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LSTM Dereverberation - Steps 5 & 6: Inference & Reconstruction\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # DIRECT PATHS - NO EXPLORATION\n",
        "    test_reverb_path = '/content/drive/MyDrive/dereverberation_dataset/test/reverberant'\n",
        "    output_folder = '/content/drive/MyDrive/dereverberation_dataset/test/output'\n",
        "    checkpoint_dir = '/content/drive/MyDrive/dereverberation_dataset/dereverberation_checkpoints'\n",
        "\n",
        "    # Create output folder\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Get audio files directly\n",
        "    test_audio_files = []\n",
        "    for f in os.listdir(test_reverb_path):\n",
        "        if f.lower().endswith(('.wav', '.flac', '.mp3', '.m4a', '.ogg')):\n",
        "            test_audio_files.append(f)\n",
        "\n",
        "    if not test_audio_files:\n",
        "        print(\"‚ùå No audio files found in test/reverberant folder\")\n",
        "        print(\"Please add audio files to proceed\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Found {len(test_audio_files)} audio files\")\n",
        "\n",
        "        # Check for trained models\n",
        "        checkpoint_files = []\n",
        "        for f in os.listdir(checkpoint_dir):\n",
        "            if f.endswith('.pt'):\n",
        "                checkpoint_files.append(os.path.join(checkpoint_dir, f))\n",
        "\n",
        "        if not checkpoint_files:\n",
        "            print(\"‚ùå No trained model found\")\n",
        "        else:\n",
        "            model_checkpoint = [f for f in checkpoint_files if 'best_model' in f][0]\n",
        "            print(f\"‚úÖ Using best model: {model_checkpoint}\")\n",
        "\n",
        "            # Initialize preprocessor\n",
        "            preprocessor = AudioPreprocessor(\n",
        "                sample_rate=16000,\n",
        "                frame_length_ms=32,\n",
        "                frame_shift_ms=8,\n",
        "                n_fft=512,\n",
        "                normalize=True\n",
        "            )\n",
        "\n",
        "            # Initialize inference pipeline\n",
        "            inference = DereverberationInference(\n",
        "                model_checkpoint_path=model_checkpoint,\n",
        "                preprocessor=preprocessor\n",
        "            )\n",
        "\n",
        "            # Process all test files\n",
        "            for audio_file in test_audio_files:\n",
        "                input_audio_path = os.path.join(test_reverb_path, audio_file)\n",
        "                output_filename = f\"enhanced_{os.path.splitext(audio_file)[0]}.wav\"\n",
        "                output_path = os.path.join(output_folder, output_filename)\n",
        "\n",
        "                print(f\"\\nüéØ Enhancing: {audio_file}\")\n",
        "                enhanced_audio = inference.enhance_audio(input_audio_path)\n",
        "                inference.save_audio(enhanced_audio, output_path)\n",
        "\n",
        "            print(f\"\\nüéâ ALL TEST FILES PROCESSED!\")\n",
        "            print(f\"Output saved to: {output_folder}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEPS 5 & 6 COMPLETE\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrV9CSf7JwsD",
        "outputId": "e9976a08-cfc0-47f8-d553-9d9ce52e75a2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "LSTM Dereverberation - Steps 5 & 6: Inference & Reconstruction\n",
            "============================================================\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚ùå No audio files found in test/reverberant folder\n",
            "Please add audio files to proceed\n",
            "\n",
            "============================================================\n",
            "STEPS 5 & 6 COMPLETE\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}
